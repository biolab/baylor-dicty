{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OvR logistic regression with weights adjusted based on weights of other phenotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'stages_DE.OvR' from '/home/khrovatin/git/baylor-dicty/stages_DE/OvR.py'>"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from importlib import reload\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0,module_path)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support,roc_auc_score\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "import sklearn.preprocessing as pp\n",
    "#alt.renderers.enable('notebook')\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "import stages_DE.stages_library\n",
    "import importlib\n",
    "importlib.reload(stages_DE.stages_library)\n",
    "\n",
    "from networks.functionsDENet import loadPickle,savePickle\n",
    "from stages_DE.stages_library import PHENOTYPES, PHENOTYPES_X, summary_classification, summary_classification_print_sort, scatter_catgory\n",
    "import stages_DE.OvR as OvR\n",
    "importlib.reload(OvR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "proteus=True\n",
    "if proteus:\n",
    "    pathClassification = '/home/khrovatin/timeTrajectoriesNet/data/stages/classification/'\n",
    "    dataPath= '/home/khrovatin/timeTrajectoriesNet/data/RPKUM/'\n",
    "else:\n",
    "    pathClassification = '/home/karin/Documents/timeTrajectories/data/stages/classification/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = pd.read_csv(dataPath + 'mergedGenes_RPKUM.tsv', sep='\\t', index_col=0)\n",
    "conditions = pd.read_csv(dataPath + 'conditions_mergedGenes.tsv', sep='\\t', index_col=None)\n",
    "\n",
    "# Retain only samples with annotations\n",
    "Y = conditions[(conditions[PHENOTYPES] != 0).any(axis=1)]\n",
    "X = genes[Y.Measurment].T.values\n",
    "#Y = conditions.query('Group ==\"WT\"')[(conditions.query('Group ==\"WT\"')[PHENOTYPES] != 0).any(axis=1)]\n",
    "#X = genes[Y.Measurment].T.values\n",
    "\n",
    "# Remove targets with too little positive samples\n",
    "order=['no_agg','disappear', 'stream', 'lag', 'tag', 'tip', 'slug', 'mhat', 'cul', 'FB']\n",
    "#order=['no_agg', 'stream', 'lag', 'tag',  'slug', 'mhat', 'cul', 'FB']\n",
    "Y = Y[order].values\n",
    "\n",
    "# Remove constant features\n",
    "X=X[:,(X.std(axis=0)!=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in train and test as was done for 5-fold cross validation, but using only 1 fold for now\n",
    "X_train, Y_train, X_test, Y_test = iterative_train_test_split(X, Y, test_size=0.2)\n",
    "#Scale X features to [0,1], use X_train scaller to also scale X_test\n",
    "scaler = pp.MinMaxScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train unmodified OvR for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight='balanced',\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=100,\n",
       "                                                 multi_class='auto', n_jobs=20,\n",
       "                                                 penalty='none',\n",
       "                                                 random_state=None,\n",
       "                                                 solver='saga', tol=0.0001,\n",
       "                                                 verbose=0, warm_start=False),\n",
       "                    n_jobs=10)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = OvR.OneVsRestClassifier(estimator=LogisticRegression(n_jobs=20,  solver='saga',penalty='none',\n",
    "                                            class_weight='balanced'\n",
    "                                            #,warm_start=True,max_iter=1\n",
    "                                             ), n_jobs=Y_train.shape[1])\n",
    "classifier.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train modified weights model - adjust weights based on weights for other phenotypes. Train sub-models so that in the middle of the training the weights are adjusted after each itteration based on weight of other sub-models/phenotypes. Currently this finds largest region of positive weights (across targets/phenotypes, according to weight sum) for each gene and then downvotes (divides) any positive weiths (of other phenotypes) that are outside of this region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Re)setting estimators\n"
     ]
    }
   ],
   "source": [
    "classifier_ordered = OvR.OneVsRestClassifier(estimator=LogisticRegression(n_jobs=20,  solver='saga',penalty='none',\n",
    "                                            class_weight='balanced'\n",
    "                                            ,warm_start=True,max_iter=1\n",
    "                                             ), n_jobs=Y_train.shape[1],warm_start=True)\n",
    "\n",
    "max=100\n",
    "for i in range(max):\n",
    "    classifier_ordered.fit(X_train,Y_train)\n",
    "    if 5 < i < max-5:\n",
    "        for feature_idx in range(X_train.shape[1]):\n",
    "            start=0\n",
    "            end=0\n",
    "            peaks=[]\n",
    "            running=False\n",
    "            curr_peak=0\n",
    "            positive_coefs=[]\n",
    "            for target_idx in range(Y_train.shape[1]):\n",
    "                coef=classifier_ordered.estimators_[target_idx].coef_[0][feature_idx]\n",
    "                if coef > 0:\n",
    "                    positive_coefs.append(target_idx)\n",
    "                    if not running:\n",
    "                        running=True\n",
    "                        start=target_idx\n",
    "                    end=target_idx\n",
    "                    curr_peak+=coef\n",
    "                elif coef < 0 and running:\n",
    "                    running=False\n",
    "                    peaks.append((start,end,curr_peak))\n",
    "            if len(peaks)>0:\n",
    "                best_peak = sorted(peaks, key=lambda tup: tup[2])[-1]\n",
    "                modify_down=[idx for idx in positive_coefs if idx < best_peak[0] or idx > best_peak[1]]\n",
    "                for modify_idx in modify_down:\n",
    "                    coef=classifier_ordered.estimators_[modify_idx].coef_[0][feature_idx]\n",
    "                    classifier_ordered.estimators_[modify_idx].coef_[0][feature_idx]=coef/2\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare weights of individual genes in both models\n",
    "c=classifier_ordered\n",
    "plt.hlines(0,0,len(c.estimators_)-1)\n",
    "# Find a gene with high weight for a target in modified model\n",
    "for i in range(X.shape[1]):\n",
    "#for i in [1000]:\n",
    "    if c.estimators_[2].coef_[0][i]>0.01:\n",
    "        #Plot weights of both models\n",
    "        for e in range(len(c.estimators_)):\n",
    "            plt.scatter(e,c.estimators_[e].coef_[0][i],c='b',alpha=0.5)\n",
    "            plt.scatter(e,classifier.estimators_[e].coef_[0][i],c='r',alpha=0.5)\n",
    "        print(i,genes[genes.std(axis=1)!=0].index[i])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmodified model\n",
      "   precision  recall  F_score  support      Group\n",
      "0       0.90    0.84     0.87     31.0     no_agg\n",
      "1       0.67    0.50     0.57      4.0  disappear\n",
      "2       0.78    0.64     0.70     11.0     stream\n",
      "3       0.76    0.76     0.76     17.0        lag\n",
      "4       0.60    0.69     0.64     13.0        tag\n",
      "5       0.60    0.60     0.60      5.0        tip\n",
      "6       0.88    0.64     0.74     11.0       slug\n",
      "7       0.60    1.00     0.75      3.0       mhat\n",
      "8       0.75    1.00     0.86      6.0        cul\n",
      "9       1.00    0.80     0.89      5.0         FB\n",
      "\n",
      "Modified weights model\n",
      "   precision  recall  F_score  support      Group\n",
      "0       0.93    0.87     0.90     31.0     no_agg\n",
      "1       0.31    1.00     0.47      4.0  disappear\n",
      "2       0.62    0.73     0.67     11.0     stream\n",
      "3       0.86    0.35     0.50     17.0        lag\n",
      "4       0.40    0.92     0.56     13.0        tag\n",
      "5       0.19    0.80     0.31      5.0        tip\n",
      "6       0.34    0.91     0.50     11.0       slug\n",
      "7       0.00    0.00     0.00      3.0       mhat\n",
      "8       0.67    1.00     0.80      6.0        cul\n",
      "9       0.50    0.80     0.62      5.0         FB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khrovatin/miniconda3/envs/env1/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print('Unmodified model')\n",
    "Y_predicted=classifier.predict(X_test)\n",
    "prfs=pd.DataFrame(precision_recall_fscore_support(Y_test, Y_predicted),index=['precision','recall','F_score','support']).T\n",
    "prfs['Group']=order\n",
    "print(prfs.round(2))\n",
    "print('\\nModified weights model')\n",
    "Y_predicted=classifier_ordered.predict(X_test)\n",
    "prfs=pd.DataFrame(precision_recall_fscore_support(Y_test, Y_predicted),index=['precision','recall','F_score','support']).T\n",
    "prfs['Group']=order\n",
    "print(prfs.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance of falsely predicted labels to true labels\n",
    "For each sample that has at least some labels calculate the distance to the closest true label of FP and closest TP of FN. Average this over all FP/FN. It would be desired that: 1.) FP would be close to the real label (low FP distance). 2.) FN would be away from the closest TP (high FN distance) - The only FN would be those that are not likely based on the TP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_error(Y_test,Y_predicted):\n",
    "    wrong_total=0\n",
    "    n_wrong=0\n",
    "    missing_total=0\n",
    "    n_missing=0\n",
    "    order_arr=np.array(order)\n",
    "    for row_idx in range(Y_test.shape[0]):\n",
    "        y_test=Y_test[row_idx,:]\n",
    "        # Use only samples with at least some ground truth positive labels and some wrongly predicted\n",
    "        if y_test.sum()>0:\n",
    "            y_predicted=Y_predicted[row_idx,:]\n",
    "            if (y_predicted!=y_test).any():\n",
    "                #print('***********')\n",
    "                #print(y_test.astype('int'))\n",
    "                #print(y_predicted)\n",
    "                # Which phenotypes were predicted/are in fact present\n",
    "                targets=order_arr[y_test==1]\n",
    "                predicted_targets=order_arr[y_predicted==1]\n",
    "                true_x=[PHENOTYPES_X[phenotype] for phenotype in targets]\n",
    "                predicted_x=[PHENOTYPES_X[phenotype] for phenotype in predicted_targets]\n",
    "                # Find closest actuall lable to the FP\n",
    "                for x in predicted_x:\n",
    "                    if x not in true_x:\n",
    "                        n_wrong+=1\n",
    "                        min_diff=np.inf\n",
    "                        for x_true in true_x:\n",
    "                            diff=abs(x-x_true)\n",
    "                            if diff<min_diff:\n",
    "                                min_diff=diff\n",
    "                        wrong_total+=min_diff\n",
    "                # Find closest TP label to the FN\n",
    "                for x in true_x:\n",
    "                    if x not in predicted_x and len(predicted_x)>0:\n",
    "                        n_missing+=1\n",
    "                        min_diff=np.inf\n",
    "                        for x_predicted in predicted_x:\n",
    "                            diff=abs(x-x_predicted)\n",
    "                            if diff<min_diff:\n",
    "                                min_diff=diff\n",
    "                        missing_total+=min_diff\n",
    "\n",
    "    print('Average distance of missing annotations (FN) to the closest TP one:',round(missing_total/n_missing,2))     \n",
    "    print('Average distance of wrong annotations (FP) to the closest true one:',round(wrong_total/n_wrong,2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmodified model:\n",
      "Average distance of missing annotations (FN) to the closest TP one: 2.0\n",
      "Average distance of wrong annotations (FP) to the closest true one: 1.5\n",
      "\n",
      "Modified weights model:\n",
      "Average distance of missing annotations (FN) to the closest TP one: 2.0\n",
      "Average distance of wrong annotations (FP) to the closest true one: 1.78\n"
     ]
    }
   ],
   "source": [
    "print('Unmodified model:')\n",
    "Y_predicted=classifier.predict(X_test)\n",
    "distance_error(Y_test,Y_predicted)\n",
    "print('\\nModified weights model:')\n",
    "Y_predicted=classifier_ordered.predict(X_test)\n",
    "distance_error(Y_test,Y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The algorithm, as currently implemented, performs worse than unmodified model - both based on OvR F score and distance of false predictions to the truth. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constrain weights to positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rpy2.robjects.packages import importr\n",
    "from rpy2 import  robjects\n",
    "from rpy2.robjects import pandas2ri \n",
    "from rpy2.rinterface_lib.embedded import RRuntimeError\n",
    "from sklearn.utils.class_weight import compute_sample_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = importr('base')\n",
    "utils = importr('utils')\n",
    "glmnet=importr('glmnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "  |======================================================================| 100%\n",
      "1\n",
      "  |======================================================================| 100%\n",
      "2\n",
      "  |======================================================================| 100%\n",
      "3\n",
      "  |======================================================================| 100%\n",
      "4\n",
      "  |======================================================================| 100%\n",
      "5\n",
      "  |======================================================================| 100%\n",
      "6\n",
      "  |======================================================================| 100%\n",
      "7\n",
      "  |======================================================================| 100%\n",
      "8\n",
      "  |======================================================================| 100%\n",
      "9\n",
      "  |======================================================================| 100%\n"
     ]
    }
   ],
   "source": [
    "Y_predicted=[]\n",
    "N_features=[]\n",
    "for target_idx in range(Y.shape[1]):\n",
    "    print(target_idx)\n",
    "    weights=compute_sample_weight('balanced',Y_train[:,target_idx])\n",
    "\n",
    "    rX_train = robjects.r.matrix(robjects.FloatVector(X_train.T.ravel()), nrow=X_train.shape[0],\n",
    "                                 ncol=X_train.shape[1])\n",
    "    rY_train=robjects.FactorVector(pd.Series(Y_train[:,target_idx]).astype('str').values)\n",
    "    rX_test = robjects.r.matrix(robjects.FloatVector(X_test.T.ravel()), nrow=X_test.shape[0],\n",
    "                                ncol=X_test.shape[1])\n",
    "    rweights=robjects.FloatVector(weights.ravel())\n",
    "\n",
    "    fit=glmnet.glmnet(rX_train,rY_train,family = 'binomial',weights=rweights,alpha=1,\n",
    "                      **{'lambda':0,'lower.limits':0,'standardize':False,'trace.it':1})\n",
    "\n",
    "    coefs=pandas2ri.rpy2py(robjects.r.matrix(fit.rx2['beta']))\n",
    "\n",
    "    N_features.append((coefs>0).sum())\n",
    "\n",
    "    Y_p=robjects.r.predict(fit,rX_test,type=\"response\",s=1)\n",
    "\n",
    "    Y_p=pandas2ri.rpy2py(Y_p)\n",
    "\n",
    "    Y_predicted_target=Y_p.copy()\n",
    "    Y_predicted_target[Y_predicted_target>0.5]=1\n",
    "    Y_predicted_target[Y_predicted_target<=0.5]=0\n",
    "    Y_predicted.append(Y_predicted_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multilabel-indicator and binary targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-528-367ebf1554e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprecision_recall_fscore_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_predicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/env1/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[0;32m-> 1484\u001b[0;31m                                     pos_label)\n\u001b[0m\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/env1/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1299\u001b[0m                          str(average_options))\n\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1302\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'binary'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/env1/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 90\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multilabel-indicator and binary targets"
     ]
    }
   ],
   "source": [
    "precision_recall_fscore_support(Y_test, Y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
