{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OvR logistic regression with weights adjusted based on weights of other phenotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from importlib import reload\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0,module_path)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support,roc_auc_score\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "import sklearn.preprocessing as pp\n",
    "#alt.renderers.enable('notebook')\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "import stages_DE.stages_library\n",
    "import importlib\n",
    "importlib.reload(stages_DE.stages_library)\n",
    "\n",
    "from networks.functionsDENet import loadPickle,savePickle\n",
    "from stages_DE.stages_library import PHENOTYPES, PHENOTYPES_X, summary_classification, summary_classification_print_sort, scatter_catgory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "proteus=True\n",
    "if proteus:\n",
    "    pathClassification = '/home/khrovatin/timeTrajectoriesNet/data/stages/classification/'\n",
    "    dataPath= '/home/khrovatin/timeTrajectoriesNet/data/RPKUM/'\n",
    "else:\n",
    "    pathClassification = '/home/karin/Documents/timeTrajectories/data/stages/classification/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = pd.read_csv(dataPath + 'mergedGenes_RPKUM.tsv', sep='\\t', index_col=0)\n",
    "conditions = pd.read_csv(dataPath + 'conditions_mergedGenes.tsv', sep='\\t', index_col=None)\n",
    "\n",
    "# Retain only samples with annotations\n",
    "Y = conditions[(conditions[PHENOTYPES] != 0).any(axis=1)]\n",
    "X = genes[Y.Measurment].T.values\n",
    "#Y = conditions.query('Group ==\"WT\"')[(conditions.query('Group ==\"WT\"')[PHENOTYPES] != 0).any(axis=1)]\n",
    "#X = genes[Y.Measurment].T.values\n",
    "\n",
    "# Remove targets with too little positive samples\n",
    "order=['no_agg','disappear', 'stream', 'lag', 'tag', 'tip', 'slug', 'mhat', 'cul', 'FB']\n",
    "#order=['no_agg', 'stream', 'lag', 'tag',  'slug', 'mhat', 'cul', 'FB']\n",
    "Y = Y[order].values\n",
    "\n",
    "# Remove constant features\n",
    "X=X[:,(X.std(axis=0)!=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in train and test as was done for 5-fold cross validation, but using only 1 fold for now\n",
    "X_train, Y_train, X_test, Y_test = iterative_train_test_split(X, Y, test_size=0.2)\n",
    "#Scale X features to [0,1], use X_train scaller to also scale X_test\n",
    "scaler = pp.MinMaxScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train unmodified OvR for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight='balanced',\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=100,\n",
       "                                                 multi_class='auto', n_jobs=20,\n",
       "                                                 penalty='none',\n",
       "                                                 random_state=None,\n",
       "                                                 solver='saga', tol=0.0001,\n",
       "                                                 verbose=0, warm_start=False),\n",
       "                    n_jobs=10)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = OneVsRestClassifier(estimator=LogisticRegression(n_jobs=20,  solver='saga',penalty='none',\n",
    "                                            class_weight='balanced'\n",
    "                                            #,warm_start=True,max_iter=1\n",
    "                                             ), n_jobs=Y_train.shape[1])\n",
    "classifier.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train modified weights model - adjust weights based on weights for other phenotypes. Train sub-models so that in the middle of the training the weights are adjusted after each itteration based on weight of other sub-models/phenotypes. Currently this finds largest region of positive weights (across targets/phenotypes, according to weight sum) for each gene and then downvotes (divides) any positive weiths (of other phenotypes) that are outside of this region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_ordered = OneVsRestClassifier(estimator=LogisticRegression(n_jobs=20,  solver='saga',penalty='none',\n",
    "                                            class_weight='balanced'\n",
    "                                            ,warm_start=True,max_iter=1\n",
    "                                             ), n_jobs=Y_train.shape[1])\n",
    "\n",
    "max=100\n",
    "for i in range(max):\n",
    "    classifier_ordered.fit(X_train,Y_train)\n",
    "    if 5 < i < max-5:\n",
    "        for feature_idx in range(X_train.shape[1]):\n",
    "            start=0\n",
    "            end=0\n",
    "            peaks=[]\n",
    "            running=False\n",
    "            curr_peak=0\n",
    "            positive_coefs=[]\n",
    "            for target_idx in range(Y_train.shape[1]):\n",
    "                coef=classifier_ordered.estimators_[target_idx].coef_[0][feature_idx]\n",
    "                if coef > 0:\n",
    "                    positive_coefs.append(target_idx)\n",
    "                    if not running:\n",
    "                        running=True\n",
    "                        start=target_idx\n",
    "                    end=target_idx\n",
    "                    curr_peak+=coef\n",
    "                elif coef < 0 and running:\n",
    "                    running=False\n",
    "                    peaks.append((start,end,curr_peak))\n",
    "            if len(peaks)>0:\n",
    "                best_peak = sorted(peaks, key=lambda tup: tup[2])[-1]\n",
    "                modify_down=[idx for idx in positive_coefs if idx < best_peak[0] or idx > best_peak[1]]\n",
    "                for modify_idx in modify_down:\n",
    "                    coef=classifier_ordered.estimators_[modify_idx].coef_[0][feature_idx]\n",
    "                    classifier_ordered.estimators_[modify_idx].coef_[0][feature_idx]=coef/2\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare weights of individual genes in both models\n",
    "c=classifier_ordered\n",
    "plt.hlines(0,0,len(c.estimators_)-1)\n",
    "# Find a gene with high weight for a target in modified model\n",
    "for i in range(X.shape[1]):\n",
    "#for i in [1000]:\n",
    "    if c.estimators_[2].coef_[0][i]>0.01:\n",
    "        #Plot weights of both models\n",
    "        for e in range(len(c.estimators_)):\n",
    "            plt.scatter(e,c.estimators_[e].coef_[0][i],c='b',alpha=0.5)\n",
    "            plt.scatter(e,classifier.estimators_[e].coef_[0][i],c='r',alpha=0.5)\n",
    "        print(i,genes[genes.std(axis=1)!=0].index[i])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmodified model\n",
      "   precision  recall  F_score  support      Group\n",
      "0       0.90    0.87     0.89     31.0     no_agg\n",
      "1       0.67    0.50     0.57      4.0  disappear\n",
      "2       0.78    0.64     0.70     11.0     stream\n",
      "3       0.68    0.76     0.72     17.0        lag\n",
      "4       0.75    0.69     0.72     13.0        tag\n",
      "5       0.50    0.20     0.29      5.0        tip\n",
      "6       0.88    0.64     0.74     11.0       slug\n",
      "7       0.60    1.00     0.75      3.0       mhat\n",
      "8       0.75    1.00     0.86      6.0        cul\n",
      "9       1.00    0.80     0.89      5.0         FB\n",
      "\n",
      "Modified weights model\n",
      "   precision  recall  F_score  support      Group\n",
      "0       0.88    0.90     0.89     31.0     no_agg\n",
      "1       0.18    1.00     0.31      4.0  disappear\n",
      "2       0.75    0.55     0.63     11.0     stream\n",
      "3       0.54    0.88     0.67     17.0        lag\n",
      "4       0.46    0.85     0.59     13.0        tag\n",
      "5       0.12    0.60     0.20      5.0        tip\n",
      "6       0.33    0.73     0.46     11.0       slug\n",
      "7       0.27    1.00     0.43      3.0       mhat\n",
      "8       0.67    1.00     0.80      6.0        cul\n",
      "9       0.33    0.80     0.47      5.0         FB\n"
     ]
    }
   ],
   "source": [
    "print('Unmodified model')\n",
    "Y_predicted=classifier.predict(X_test)\n",
    "prfs=pd.DataFrame(precision_recall_fscore_support(Y_test, Y_predicted),index=['precision','recall','F_score','support']).T\n",
    "prfs['Group']=order\n",
    "print(prfs.round(2))\n",
    "print('\\nModified weights model')\n",
    "Y_predicted=classifier_ordered.predict(X_test)\n",
    "prfs=pd.DataFrame(precision_recall_fscore_support(Y_test, Y_predicted),index=['precision','recall','F_score','support']).T\n",
    "prfs['Group']=order\n",
    "print(prfs.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance of falsely predicted labels to true labels\n",
    "For each sample that has at least some labels calculate the distance to the closest true label of FP and closest TP of FN. Average this over all FP/FN. It would be desired that: 1.) FP would be close to the real label (low FP distance). 2.) FN would be away from the closest TP (high FN distance) - The only FN would be those that are not likely based on the TP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_error(Y_test,Y_predicted):\n",
    "    wrong_total=0\n",
    "    n_wrong=0\n",
    "    missing_total=0\n",
    "    n_missing=0\n",
    "    order_arr=np.array(order)\n",
    "    for row_idx in range(Y_test.shape[0]):\n",
    "        y_test=Y_test[row_idx,:]\n",
    "        # Use only samples with at least some ground truth positive labels and some wrongly predicted\n",
    "        if y_test.sum()>0:\n",
    "            y_predicted=Y_predicted[row_idx,:]\n",
    "            if (y_predicted!=y_test).any():\n",
    "                #print('***********')\n",
    "                #print(y_test.astype('int'))\n",
    "                #print(y_predicted)\n",
    "                # Which phenotypes were predicted/are in fact present\n",
    "                targets=order_arr[y_test==1]\n",
    "                predicted_targets=order_arr[y_predicted==1]\n",
    "                true_x=[PHENOTYPES_X[phenotype] for phenotype in targets]\n",
    "                predicted_x=[PHENOTYPES_X[phenotype] for phenotype in predicted_targets]\n",
    "                # Find closest actuall lable to the FP\n",
    "                for x in predicted_x:\n",
    "                    if x not in true_x:\n",
    "                        n_wrong+=1\n",
    "                        min_diff=np.inf\n",
    "                        for x_true in true_x:\n",
    "                            diff=abs(x-x_true)\n",
    "                            if diff<min_diff:\n",
    "                                min_diff=diff\n",
    "                        wrong_total+=min_diff\n",
    "                # Find closest TP label to the FN\n",
    "                for x in true_x:\n",
    "                    if x not in predicted_x and len(predicted_x)>0:\n",
    "                        n_missing+=1\n",
    "                        min_diff=np.inf\n",
    "                        for x_predicted in predicted_x:\n",
    "                            diff=abs(x-x_predicted)\n",
    "                            if diff<min_diff:\n",
    "                                min_diff=diff\n",
    "                        missing_total+=min_diff\n",
    "\n",
    "    print('Average distance of missing annotations (FN) to the closest TP one:',round(missing_total/n_missing,2))     \n",
    "    print('Average distance of wrong annotations (FP) to the closest true one:',round(wrong_total/n_wrong,2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmodified model:\n",
      "Average distance of missing annotations (FN) to the closest TP one: 1.85\n",
      "Average distance of wrong annotations (FP) to the closest true one: 1.62\n",
      "\n",
      "Modified weights model:\n",
      "Average distance of missing annotations (FN) to the closest TP one: 2.0\n",
      "Average distance of wrong annotations (FP) to the closest true one: 1.74\n"
     ]
    }
   ],
   "source": [
    "print('Unmodified model:')\n",
    "Y_predicted=classifier.predict(X_test)\n",
    "distance_error(Y_test,Y_predicted)\n",
    "print('\\nModified weights model:')\n",
    "Y_predicted=classifier_ordered.predict(X_test)\n",
    "distance_error(Y_test,Y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The algorithm, as currently implemented, performs worse than unmodified model - both based on OvR F score and distance of false predictions to the truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
